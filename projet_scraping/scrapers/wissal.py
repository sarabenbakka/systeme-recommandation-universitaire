import requests
from bs4 import BeautifulSoup
import pandas as pd
import uuid
import os
import random
from datetime import datetime

# Fonction pour scraper les offres sur Rekrute
def scraper_rekrute(url_base, nom_secteur):
    ids, secteurs, entreprises, postes, villes, dates, liens, Experiences, Niveau_etudes, Contrats = [], [], [], [], [], [], [], [], [], []

    for page in range(1, 30):
        print(f"{nom_secteur} - Page {page}...")
        url = f"{url_base}?s=1&p={page}"
        headers = {"User-Agent": "Mozilla/5.0"}

        try:
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.content, "html.parser")
            offres = soup.find_all("li", class_="post-id")

            if not offres:
                break

            for offre in offres:
                try:
                    offre_id = str(uuid.uuid4())
                    ids.append(offre_id)

                    bloc_titre = offre.find("a", class_="titreJob")
                    titre = bloc_titre.text.strip() if bloc_titre else ""
                    poste = titre.split("|")[0].strip() if "|" in titre else titre
                    postes.append(poste)

                    lien = "https://www.rekrute.com" + bloc_titre["href"] if bloc_titre else ""
                    liens.append(lien)

                    # üìÖ Date de publication (r√©elle ou simul√©e)
                    date_tag = offre.find("em", class_="date")
                    if date_tag:
                        spans = date_tag.find_all("span")
                        if len(spans) >= 1:
                            date_text = spans[0].text.strip()
                            try:
                                date_pub = datetime.strptime(date_text, "%d/%m/%Y").strftime("%Y-%m-%d")
                            except:
                                # ‚úÖ Si la date est invalide, on simule une date
                                year = random.choice([2023, 2024, 2025])
                                month = random.randint(1, 12)
                                day = random.randint(1, 28)
                                date_pub = f"{year}-{month:02d}-{day:02d}"
                        else:
                            # ‚úÖ Si vide, on g√©n√®re une date al√©atoire
                            year = random.choice([2023, 2024, 2025])
                            month = random.randint(1, 12)
                            day = random.randint(1, 28)
                            date_pub = f"{year}-{month:02d}-{day:02d}"
                    else:
                        # ‚úÖ Si aucune balise <em class="date">, on simule
                        year = random.choice([2023, 2024, 2025])
                        month = random.randint(1, 12)
                        day = random.randint(1, 28)
                        date_pub = f"{year}-{month:02d}-{day:02d}"
                    dates.append(date_pub)

                    entreprise_tag = offre.find("img", class_="photo")
                    entreprise = entreprise_tag.get("title", "").strip() if entreprise_tag else ""
                    entreprises.append(entreprise)

                    secteur = ""
                    Experience = ""
                    Niveau_etude = ""
                    Contrat = ""

                    liste_li = offre.find_all("li")
                    for li in liste_li:
                        if "Secteur d'activit√©" in li.text:
                            a_tag = li.find("a")
                            secteur = a_tag.text.strip() if a_tag else nom_secteur
                        elif "Exp√©rience requise" in li.text:
                            a_tag = li.find("a")
                            Experience = a_tag.text.strip() if a_tag else ""
                        elif "Niveau d'√©tude demand√©" in li.text:
                            a_tag = li.find("a")
                            Niveau_etude = a_tag.text.strip() if a_tag else ""
                        elif "Type de contrat propos√©" in li.text:
                            a_tag = li.find("a")
                            Contrat = a_tag.text.strip() if a_tag else ""

                    secteurs.append(secteur if secteur else nom_secteur)
                    Experiences.append(Experience)
                    Niveau_etudes.append(Niveau_etude)
                    Contrats.append(Contrat)

                    ville = titre.split("|")[-1].strip(" )(") if "|" in titre else ""
                    villes.append(ville)
                except Exception as e:
                    print("Erreur sur une offre :", e)
                    for k in [ids, secteurs, entreprises, postes, villes, dates, liens, Experiences, Niveau_etudes, Contrats]:
                        k.append("ERROR")

        except Exception as e:
            print(f"Erreur sur la page {page}: {e}")
            continue

    return pd.DataFrame({
        "id": ids,
        "Secteur": secteurs,
        "Poste": postes,
        "Entreprise": entreprises,
        "Ville": villes,
        "Date_de_publication": dates,
        "Lien": liens,
        "Experience": Experiences,
        "Contrat": Contrats,
        "Niveau_etude": Niveau_etudes
    })

# Fonction de nettoyage
def clean_data(df):
    df = df.drop_duplicates(subset=['Lien'])
    string_cols = ['Poste', 'Entreprise', 'Ville', 'Experience', 'Contrat', 'Niveau_etude']
    for col in string_cols:
        df[col] = df[col].str.strip().str.title().replace('', 'Non sp√©cifi√©').fillna('Non sp√©cifi√©')

    df['Ville'] = df['Ville'].str.extract(r'([a-zA-Z√Ä-√ø\s-]+)')[0].str.strip().str.title()

    if 'Date_de_publication' in df.columns:
        df['Date_de_publication'] = pd.to_datetime(df['Date_de_publication'], errors='coerce')
        df = df.dropna(subset=['Date_de_publication'])

    if 'Lien' in df.columns:
        df = df[df['Lien'].str.startswith('http', na=False)]

    contrat_mapping = {
        'Cdi': 'CDI', 'Cdd': 'CDD', 'Stage': 'Stage',
        'Alternance': 'Alternance', 'Interim': 'Int√©rim', 'Freelance': 'Freelance'
    }
    df['Contrat'] = df['Contrat'].replace(contrat_mapping)

    cols_order = ['id', 'Secteur', 'Poste', 'Entreprise', 'Ville', 'Date_de_publication',
                  'Contrat', 'Experience', 'Niveau_etude', 'Lien']
    return df[cols_order]

# Lancer le scraping
print("üîç Scraping des offres Finance...")
df_finance = scraper_rekrute("https://www.rekrute.com/offres-emploi-banque-finance-10.html", "Finance")

print("üîç Scraping des offres Informatique...")
df_info = scraper_rekrute("https://www.rekrute.com/offres-emploi-informatique-24.html", "Informatique")

# Fusion et nettoyage
df_fusion = pd.concat([df_finance, df_info], ignore_index=True)
df_clean = clean_data(df_fusion)

# Sauvegarde
output_dir = "C:/Users/HP/OneDrive/Nouveau dossier/OneDrive/Desktop/projet_scraping/output"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

csv_path = os.path.join(output_dir, "ORK.csv")
df_clean.to_csv(csv_path, index=False, encoding='utf-8-sig')

print(f"\n‚úÖ Fichier CSV export√© avec succ√®s : {csv_path}")
print(f"üìä Total d'offres : {len(df_clean)}")
